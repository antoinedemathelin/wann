{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WANN experiments on Sentiment Analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.constraints import MinMaxNorm\n",
    "\n",
    "sys.path.append(\"../wann\")\n",
    "from utils import sa, BaggingModels, cross_val\n",
    "from sa_experiments import run_sa_experiments\n",
    "from methods import *\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 domains: electronics, books, dvd, kitchen\n",
    "source = 'electronics'\n",
    "target = 'dvd'\n",
    "\n",
    "N = 50   # Number of labeled target data\n",
    "m = 700  # Number of labeled source data\n",
    "n = 700  # Number of unlabeled target data\n",
    "\n",
    "X, y, src_index, tgt_index = sa(source, target)\n",
    "mu = y[src_index].mean(); std = y[src_index].std()\n",
    "y = (y-y[src_index].mean())/y[src_index].std()\n",
    "shape = X.shape[1]\n",
    "\n",
    "np.random.seed(0)\n",
    "src_index = np.random.choice(src_index, m, replace=False)\n",
    "tgt_index, tgt_test_index = train_test_split(tgt_index, train_size=n, test_size=1000)\n",
    "tgt_train_index = np.random.choice(tgt_index, N, replace=False)\n",
    "train_index = np.concatenate((src_index, tgt_train_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(shape, activation=None, C=1, name=\"BaseModel\"):\n",
    "    inputs = Input(shape=(shape,))\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    modeled = Dropout(0.5)(modeled)\n",
    "    modeled = Dense(10, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    modeled = Dropout(0.2)(modeled)\n",
    "    modeled = Dense(1, activation=activation,\n",
    "                    kernel_constraint=MinMaxNorm(0, C),\n",
    "                    bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    model = Model(inputs, modeled, name=name)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_encoder(shape, C=1, name=\"encoder\"):\n",
    "    inputs = Input(shape=(shape,))\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    modeled = Dropout(0.5)(modeled)\n",
    "    modeled = Dense(10, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    modeled = Dropout(0.2)(modeled)\n",
    "    model = Model(inputs, modeled)\n",
    "    model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_task(shape, C=1, activation=None, name=\"task\"):\n",
    "    inputs = Input(shape=(shape,))\n",
    "    modeled = Dense(1, activation=activation,\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    model = Model(inputs, modeled)\n",
    "    model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "base_estimator = BaggingModels(func=get_base_model,\n",
    "                               n_models=1,\n",
    "                               n_jobs=None,\n",
    "                               shape=shape,\n",
    "                               C=1,\n",
    "                               random_state=0)\n",
    "fit_params = dict(epochs=200,\n",
    "                  batch_size=64,\n",
    "                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target score: 0.997\n"
     ]
    }
   ],
   "source": [
    "no_reweight = copy.deepcopy(base_estimator)\n",
    "no_reweight.fit(X[train_index], y[train_index], **fit_params)\n",
    "\n",
    "y_pred = no_reweight.predict(X)\n",
    "score= mean_squared_error(y[tgt_test_index], y_pred[tgt_test_index])\n",
    "print('Target score: %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrAdaBoostR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv error of estimator 0: 0.937 (0.2184746222)\n",
      "cv error of estimator 1: 0.967 (0.2514102849)\n",
      "cv error of estimator 2: 1.041 (0.3927827499)\n",
      "cv error of estimator 3: 1.164 (0.5074053409)\n",
      "cv error of estimator 4: 1.238 (0.6250339594)\n",
      "cv error of estimator 5: 1.295 (0.7800075651)\n",
      "cv error of estimator 6: 1.289 (0.8230790703)\n",
      "cv error of estimator 7: 1.155 (0.7169947271)\n",
      "cv error of estimator 8: 0.993 (0.5506487518)\n",
      "cv error of estimator 9: 0.982 (0.4590033025)\n",
      "Target score: 0.988\n"
     ]
    }
   ],
   "source": [
    "tradaboost = TwoStageTrAdaBoostR2(func=get_base_model,\n",
    "                                  verbose=1,\n",
    "                                  n_jobs=None,\n",
    "                                  C=1,\n",
    "                                  random_state=0,\n",
    "                                  shape=X.shape[1])\n",
    "tradaboost.fit(X, y, [src_index, tgt_train_index], **fit_params)\n",
    "y_pred = tradaboost.predict(X)\n",
    "score= mean_squared_error(y[tgt_test_index], y_pred[tgt_test_index])\n",
    "print('Target score: %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target score: 1.087\n"
     ]
    }
   ],
   "source": [
    "kmm = KMM(base_estimator)\n",
    "kmm.fit(X, y, index=[src_index, tgt_index, tgt_train_index], **fit_params)\n",
    "\n",
    "y_pred = kmm.predict(X)\n",
    "score= mean_squared_error(y[tgt_test_index], y_pred[tgt_test_index])\n",
    "print('Target score: %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLIEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target score: 0.996\n"
     ]
    }
   ],
   "source": [
    "kliep = KLIEP(base_estimator)\n",
    "kliep.fit(X, y, index=[src_index, tgt_index, tgt_train_index], **fit_params)\n",
    "\n",
    "y_pred = kliep.predict(X)\n",
    "score= mean_squared_error(y[tgt_test_index], y_pred[tgt_test_index])\n",
    "print('Target score: %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target score: 1.150\n"
     ]
    }
   ],
   "source": [
    "dann = BaggingModels(DANN, n_models=1, n_jobs=None, random_state=0,\n",
    "                     get_encoder=get_encoder, get_task=get_task, lambda_=0.02)\n",
    "\n",
    "dann.fit(X, y, index=[src_index, tgt_index, tgt_train_index], **fit_params)\n",
    "\n",
    "y_pred = dann.predict(X)\n",
    "score = mean_squared_error(y[tgt_test_index], y_pred[tgt_test_index])\n",
    "print('Target score: %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target score: 0.963\n"
     ]
    }
   ],
   "source": [
    "wann = BaggingModels(WANN, n_models=1, n_jobs=None, random_state=0,\n",
    "                     get_base_model=get_base_model, C=1, C_w=0.2)\n",
    "\n",
    "wann.fit(X, y, index=[src_index, tgt_train_index], **fit_params)\n",
    "\n",
    "y_pred = wann.predict(X)\n",
    "score = mean_squared_error(y[tgt_test_index], y_pred[tgt_test_index])\n",
    "print('Target score: %.3f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Cw Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: param = 0.010 | score = 1.0933\n",
      "Cross Validation: param = 0.020 | score = 1.0744\n",
      "Cross Validation: param = 0.050 | score = 1.0392\n",
      "Cross Validation: param = 0.100 | score = 1.0090\n",
      "Cross Validation: param = 0.200 | score = 0.9978\n",
      "Cross Validation: param = 0.500 | score = 1.0255\n",
      "Cross Validation: param = 1.000 | score = 1.0535\n",
      "Best: param = 0.200 | score = 0.9978\n"
     ]
    }
   ],
   "source": [
    "cross_val(\"WANN\", X, y, src_index, None, tgt_train_index,\n",
    "          params=[0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1],\n",
    "          fit_params=fit_params, cv=5,\n",
    "          get_base_model=get_base_model, C=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment for method: WANN\n",
      "\n",
      "\n",
      "############# dvd #############\n",
      "--------- books ----------\n",
      "Target_score: 0.990\n",
      "--------- electronics ----------\n",
      "Target_score: 0.897\n",
      "--------- kitchen ----------\n",
      "Target_score: 0.977\n",
      "############# books #############\n",
      "--------- dvd ----------\n",
      "Target_score: 1.039\n",
      "--------- electronics ----------\n",
      "Target_score: 1.046\n",
      "--------- kitchen ----------\n",
      "Target_score: 1.019\n",
      "############# electronics #############\n",
      "--------- dvd ----------\n",
      "Target_score: 0.963\n",
      "--------- books ----------\n",
      "Target_score: 1.000\n",
      "--------- kitchen ----------\n",
      "Target_score: 0.823\n",
      "############# kitchen #############\n",
      "--------- dvd ----------\n",
      "Target_score: 0.931\n",
      "--------- books ----------\n",
      "Target_score: 1.082\n",
      "--------- electronics ----------\n",
      "Target_score: 0.834\n"
     ]
    }
   ],
   "source": [
    "df = run_sa_experiments(method=\"WANN\",\n",
    "                        get_base_model=get_base_model,\n",
    "                        get_task=get_task,\n",
    "                        get_encoder=get_encoder,\n",
    "                        C=1,\n",
    "                        C_w=0.2,\n",
    "                        lambda_=0.1,\n",
    "                        sigma=0.1,\n",
    "                        epochs=200,\n",
    "                        batch_size=64,\n",
    "                        n_models=1,\n",
    "                        n_jobs=None,\n",
    "                        n_source=700,\n",
    "                        n_target_unlabeled=700,\n",
    "                        n_target_labeled=50,\n",
    "                        n_target_test=1000,\n",
    "                        random_state=0,\n",
    "                        save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch all experiments (all methods, 10 times)\n",
    "Uncomment cell below to launch experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ..\\wann\\sa_experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wann",
   "language": "python",
   "name": "wann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
